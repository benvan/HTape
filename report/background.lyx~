#LyX 1.6.5 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\use_default_options true
\language english
\inputencoding auto
\font_roman palatino
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
HTAPE: Head-Tracked Audio Processing Engine
\end_layout

\begin_layout Author
Ben van Enckevort
\end_layout

\begin_layout Date
June 2011
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Abstract
Interest in virtual spatialisation of sound has piqued over the last two
 decades, driven in part by efforts from the video games industry.
 However, despite the development of audio filters designed to provide spatial
 information to the listener, the gamer's ability to localise sound presented
 over headphones remains dubious at best.
 Common implementations allow for some degree of positional accuracy, but
 front-back and elevation confusions are commonplace.
 Normally, one would rotate their head and observe how a sound changes in
 order to pinpoint it's location, but this is impossible using headphones,
 since the sound-scape unnaturally follows the listener wherever he turns.
 This project describes a low-cost and unobtrusive solution to this problem,
 by tracking the listener's head using a web-cam.
 In addition to the benefits of auditory realism, we explore the extra degree
 of freedom provided to the gamer by detaching the virtual listener from
 the virtual camera.
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The aim of this project is to develop a tool is capable of presenting immersive
 3D audio to a listener over headphones, by responding in real-time to the
 motion under rotation and translation of the listener's head.
 The success of this project will be judged in part by the listener's ability
 to accurately locate the source of a virtual sound, and in part by the
 listener's evaluation of the benefit of being able to interact with the
 sound-scape without modifying the position and motion of the virtual character.
\end_layout

\begin_layout Standard
The solution should therefore be capable of playing a monaural sound as
 if it were originating from a particular location in space, and the listener
 should be able to locate the source of this sound with significantly greater
 accuracy than when attempted without the use of head tracking.
 Additionally, the tool should be capable of simulating the effects of motion
 on the sound-scape, in terms of both the listener and the sound sources.
 Finally, the tool should allow the user to choose between HRTF models,
 and allow advanced calibration in the form of HRTF blending.
\end_layout

\begin_layout Subsection
Motivation
\end_layout

\begin_layout Standard
As of the time of writing, the video games industry is the largest entertainment
 industry on the planet.
 Call of Duty: Modern Warfare 2, the most popular video game ever released
 to date, made $550 million in the first five days of it's release and over
 $1 billion subsequently.
 With this kind of monetary gain at play, it is little wonder that video
 games developers are constantly striving for the latest and greatest in
 gaming technology.
\end_layout

\begin_layout Standard
Since the humble days of Pong, Frogger and Donkey Kong, we have seen a constant
 stream of improvements made to the look, feel and realism of video games,
 but as far as technology is concerned, this drive seems to be primarily
 concerned with graphics, and more recently physics and convincing A.I, neglectin
g the importance of realistic sound.
\end_layout

\begin_layout Standard
This late decade has brought on a boom of efforts into bringing visual effects
 into the realm of the 3D, heralded as the answer to the apparent dwindling
 allure of standard flat cinema.
 In the gaming world, Nintendo plan to release the highly anticipated 3DS
 in March 2011, which boasts the ability to display “3D effects without
 the need for any special glasses”.
 Why then, one might ask, do we not see similar efforts poured into the
 ability to produce “3D” audio? Well to some extent, we have, mostly in
 the form of head-related transfer functions operating over headphones,
 discussed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Sound-localisation_HRTF"

\end_inset

.
 However, the drawbacks of attempting to spatialise sound over headphones
 are that the listener loses auditory-motor integration.
 This caveat remains an important problem, and has yet to be solved in a
 commercially viable manner, which is what we attempt to achieve here.
 By using a simple web-cam, this project proposes that head tracking can
 overcome this restriction and provide the gamer with a potentially unprecedente
d user experience.
\end_layout

\begin_layout Subsection
Relevance
\end_layout

\begin_layout Standard
This project has many applications, with the greatest potential uptake being
 in the gaming industry.
 First and foremost, the intention is to enhance immersion, which is the
 holy grail of virtual environment simulation.
 Secondly, it expands the ways in which the user can interact with a game,
 effectively decoupling the virtual camera from the virtual listener.
 For example, whilst sprinting in-game, a quick flick of the head should
 be enough to determine whether a sound somewhere to the player's left is
 located in front or behind his current position.
 Traditionally, one would achieve this by changing the orientation of the
 game character itself, but this invariably throws the character off course
 as a side-effect.
 Also, with the introduction of new ways to interact with gaming devices,
 such as the Kinect, and the PlayStation Move, being able to convincingly
 attach sound sources to real-world objects is an attractive idea.
 This is not possible without head tracking because it is otherwise impossible
 to know where the user's ears are.
 A potential application of this might be enhancing the sound effects generated
 by a virtual lightsaber, wielded by the player in the form of the PlayStation
 Move controller.
 Brushing the blade past the left and right ears by making the respective
 motion with the game controller should create the familiar buzzing sound
 on the left and right side respectively.
 In terms of immersion, this kind of interaction could provide a large payoff.
\end_layout

\begin_layout Standard
In addition to gaming, the technique can be used to enhance 3D conferencing
 environments, and could be massively instrumental in paving the way for
 low cost auditory displays for the blind and hard of sight.
\end_layout

\begin_layout Standard
It's only recently that head tracking has become affordable for the home
 user both in terms of computing power and equipment, which is possibly
 why we haven't seen commercial solutions to the problem so far.
\end_layout

\begin_layout Subsection
An important distinction
\end_layout

\begin_layout Standard
At this point, it may settle on the reader's mind that surround sound was
 invented for this purpose.
 While it is true that surround sound does provide at least part of the
 answer to this problem, it has its shortcomings.
 First of all, surround sound requires a lot of equipment, and a room to
 house it in.
 Only one player can reasonably use a surround sound system at a time, and
 even if we restrict its usage to one player, sounds played out loud in
 this fashion can be a major annoyance for other people in the room.
 It is common, therefore, for headphones to be used for gaming, and surround
 sound to be used more generally for passive entertainment to a potentially
 wider audience, as with watching a movie.
 In addition, a widespread feature of on-line gaming is the ability for
 verbal communication between players, and headsets are generally used for
 this purpose.
 Finally, it is very difficult if not currently impossible to replicate
 sound sources in close proximity to the user's head using surround sound;
 an example being a quiet whisper into one ear.
\end_layout

\begin_layout Section
Background
\end_layout

\begin_layout Subsection
Sound localisation
\end_layout

\begin_layout Standard
Sound localisation is a term which describes the ability for a listener
 to approximate the direction and distance of a perceived sound.
 Human beings make use of several auditory cues in this evaluation, including
 spectral information, interaural time difference (ITD) and interaural level
 difference (ILD).
 We characterise the location of a sound in 3D space by three quantities:
 its azimuth angle, its elevation angle and its distance from the listener
 (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Spherical-coordinate-system."

\end_inset

).
 While the exact mechanisms by which our brains evaluate these parameters
 is yet unknown, we are familiar with which audio cues are required.
 Interaural time differences exist because sound propagates at a finite
 speed.
 Sound emanating from the left will reach the left ear before it reaches
 the right, and due to sound's relatively low propagation speed through
 air, we are able to detect this.
 Interaural level differences occur primarily because the head shadows the
 furthest ear from the sound source.
 Depending on the frequency of the sound, signal dissipation in air may
 also play a detectable role in ILDs.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename attachments/localisation1.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Spherical-coordinate-system."

\end_inset

Spherical coordinate system.
 Diagram probably needs changing, since it's a bit hard to read...
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
ITD, ILD and the locus of possibilities
\end_layout

\begin_layout Standard
By considering only the ITD between left and right ears, it is possible
 to confine the potential location of a sound to the surface of a cone,
 emanating laterally from the listener's head.
 This cone, called the 
\begin_inset Quotes eld
\end_inset

cone of confusion
\begin_inset Quotes erd
\end_inset

 (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cone-of-confusion"

\end_inset

.), describes all the points in space from which a sound would share the
 same ITD.
 In his pioneering foray into the mechanics of sound localisation, Hans
 Wallach concluded that since we are able to localise in the median plane
 (the plane passing vertically through the skull, dividing the head exactly
 into left and right sides), another factor must exist
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename attachments/cone_of_confusion.gif
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Cone-of-confusion"

\end_inset

Cone of confusion.
 (http://electronics.howstuffworks.com/virtual-surround-sound.htm, 04/01/2011)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
\begin_inset CommandInset label
LatexCommand label
name "sub:Sound-localisation_HRTF"

\end_inset

Sound localisation in the median plane
\end_layout

\begin_layout Standard
The head, shoulders, torso and pinnae play a large role in our ability to
 pinpoint sound sources.
 When a sound reaches the human body, it is reflected and absorbed in different
 quantities and at different frequencies by these various parts.
 The reflections interfere with each other, each one subtly changing the
 shape of the sound wave.
 One's subconscious grows to be highly accustomed to the structure of these
 changes, to such an extent that the brain, by spectral analysis of a sound,
 can determine its approximate location in the median plane.
 This reduces the cone of confusion down to a single point, since unlike
 with ITDs and ILDs, the sound's elevation has a detectable effect.
 These frequency responses, whilst similar between individuals, are highly
 characteristic.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:HRIR-example"

\end_inset

.
 shows the measured frequency responses for 3 individuals.
 The measured frequency response at one particular relative location to
 the listener (in this example, 135° azimuth, 36° elevation) is known as
 a head-related impulse response (HRIR).
 An aggregation of HRIRs from the points on a hypothetical sphere surrounding
 the listener's head is known as the head-related transfer function (HRTF),
 and this is theoretically capable of fully describing the frequency response
 expected at the listener's head from an arbitrary point in space.
 Of course, in practice we cannot take infinite such measurements, so HRIRs
 are sampled at various points on this sphere to produce a practical HRTF.
\end_layout

\begin_layout Standard
Because of each HRTF's individuality, when a sound is recorded using one
 listener's head and played to another through headphones, one can experience
 difficulties with localisation, especially in the front-back direction.
 It is also common to experience 
\begin_inset Quotes eld
\end_inset

in-head-localisation
\begin_inset Quotes erd
\end_inset

 under these circumstances, where the sound appears to be emanating from
 inside the listener's head.
 For this reason, it is important to find an HRTF which suits the listener,
 if it is not feasible to measure the listener's own.
 This project will address this topic later.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename attachments/hrtf.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:HRIR-example"

\end_inset

HRIR for the left ears of 3 individuals (Image property of Durand R.
 Begault, 
\begin_inset Quotes eld
\end_inset

3D Sound for Virtual Reality and Multimedia
\begin_inset Quotes erd
\end_inset

, Academic Press Professional, 1994)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Distance of the sound source
\end_layout

\begin_layout Standard
While HRTFs do provide distance information to the listener, it can be limited.
 In close-up situations such as whispering into one ear, where the ILDs
 are large, we require little extra information to localise the sound.
 However, when a sound is farther away, ILDs have a much lesser effect.
 The brain therefore makes use of other auditory cues:
\end_layout

\begin_layout Enumerate
Sound Spectrum.
 High frequencies are attenuated in any medium much faster than low frequencies.
 As commonly experienced when watching news broadcasts, the sound of gunfire
 a large distance away invariably sounds more bassy and boomy than the high
 pitched crackle of gunfire near the camera, and we instinctively (and correctly
) deduce the relative distances of each, despite having zero directional
 information.
 For sounds with a recognisable spectrum, like speech, footsteps, vehicle
 noise etc., distance can be estimated with the aid of this information.
\end_layout

\begin_layout Enumerate
Volume.
 Sounds decrease in volume the farther they are received from the source.
 For known sounds, as above, the volume at which we detect them also plays
 a role in determining how far they are from us.
\end_layout

\begin_layout Enumerate
Parallax.
 As with vision, when the listener is moving, nearby sound sources appear
 to move faster relative to the listener than distant ones.
 Importantly, Hans Wallach showed in 1938 that head movement plays a vital
 role in sound localisation for human beings.
\end_layout

\begin_layout Enumerate
Reflection.
 NOTE: Not sure whether to include this.
 It counts, but it's unlikely that I'll implement it.
 Any thoughts?
\end_layout

\begin_layout Subsection
Putting this knowledge into practice
\end_layout

\begin_layout Subsubsection
Binaural recordings and their drawbacks
\end_layout

\begin_layout Standard
Binaural recordings are made by using microphones attached to or placed
 inside the ear canals of a dummy head or a real person.
 As such, they capture the spectral information originally encountered during
 the recording by the listener.
 When replayed over headphones, they can produce an incredibly realistic
 reproduction of the recording, complete with spatial information (a famous
 example, known as the 
\begin_inset Quotes eld
\end_inset

The Virtual Barbershop
\begin_inset Quotes erd
\end_inset

 can be found 
\begin_inset CommandInset href
LatexCommand href
name "here"
target "http://ccgi.bluerabbit.plus.com/~bluerabbit/virtualbarbershop"

\end_inset

).
 My own personal encounter with a tailored binaural recording had me turning
 my head and body for many seconds before I realised that playback had already
 begun.
 Additionally, since binaural recordings are recorded using one specific
 setup (be it dummy or person) but listened to by many, the second listener
 experiences a loss of positional accuracy.
 The most common of these is back-front confusion, which can lead to directional
 confusion, such as the interpretation that a virtual sound source is travelling
 anti-clockwise around the listener, when it was recorded travelling clockwise
 (NOTE: Insert diagram here?).
 Of course, at the end of the day, a binaural recording is just that - a
 static recording, so it cannot be used in a dynamic fashion.
 If dynamic usage is desired, we need to find a way to generate 
\begin_inset Quotes eld
\end_inset

binaural
\begin_inset Quotes erd
\end_inset

 sound from flat recordings.
\end_layout

\begin_layout Subsubsection
Dynamic binaural audio
\end_layout

\begin_layout Standard
At this point, we have enough information to begin decomposing the problem
 of creating dynamic 3D auditory environments.
 Given that we understand the components involved in localisation, we can
 begin to develop a model which incorporates their effects on a virtual
 sound source.
 The most important of these in terms of localisation cues is the HRTF.
\end_layout

\begin_layout Subsubsection
HRTF as a digital filter
\end_layout

\begin_layout Standard
Since an HRTF is simply a collection of (finite) impulse responses from
 spatial locations, we can use a particular impulse response in convolution
 with monaural sound to produce binaural sound that appears to have come
 from that location (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:hrtf_implementation"

\end_inset

).
 An implementation would involve delaying the output of the appropriate
 left/right channel by the ITD, and applying the appropriate FIR as a filter.
 Formally, if one wishes to convert a monaural signal 
\begin_inset Formula $x(n)$
\end_inset

 into a binaural signal using a FIRs 
\begin_inset Formula $HRIR_{l,\theta,\phi}$
\end_inset

and 
\begin_inset Formula $HRIR_{r,\theta,\phi}$
\end_inset

 for azimuth 
\begin_inset Formula $\theta$
\end_inset

 and elevation 
\begin_inset Formula $\phi$
\end_inset

, then one must construct two signals, 
\begin_inset Formula $x_{l}(n)$
\end_inset

 and 
\begin_inset Formula $x_{r}(n)$
\end_inset

 to represent the left and right channels respectively, and play these simultane
ously to the left and right ears over headphones.
 If we define the ITD, 
\begin_inset Formula $ITD_{\theta,\phi}$
\end_inset

 to be negative for sounds arriving at the left ear first, then the two
 channels can be calculated as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
x_{l}(n)=x(n-ITD_{\theta,\phi})*HRIR_{l,\theta,\phi}(n)\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \[
x_{r}(n)=x(n)*HRIR_{r,\theta,\phi}(n)\]

\end_inset


\end_layout

\begin_layout Standard
Since a virtual sound source is unrestricted in its movement, we must cater
 for the situation when there exists no HRIR for its location, which unsurprisin
gly is most of the time.
 One approach might be to find the nearest HRIR corresponding to the source's
 location, but unless the HRTF is measure at high spatial resolution, this
 would introduce perceptible boundary locations at which a smoothly travelling
 sound would appear to jump immediately from one location to the next.
 A more sensible approach would be to interpolate the filters such that
 a new filter can be generated for any given point in space.
 Linear interpolation works poorly for static localisation, because linear
 interpolation fails to pay attention to the salient structures of an impulse
 response.
 However, large errors in interpolation have been found to be tolerable
 in dynamic spatialisation, which is the method used by this project.
 The aim is to mitigate the effects of using cheap interpolation techniques
 by introducing motor-auditory integration.
\end_layout

\begin_layout Standard
As with binaural recordings, localisation errors are to be expected from
 spatialised audio.
 Hypothetically, even if we were able to provide perfect HRTF interpolation,
 listeners would still experience the common front-back and elevation localisati
on errors.
 In real life, when faced with this confusion, we tend to move our heads
 toward the expected direction of the sound.
 The difference in sound at the prior and post positions is then enough
 to clarify the position of the sound in 3D space.
 When wearing headphones, this becomes impossible, since the sound-scape
 simply follows the user's head wherever he turns.
 Despite being completely unnatural in the first place, and thus a disruption
 to the illusion of immersive audio, it also fully inhibits the ability
 to use head motion in sound localisation.
 If we can remedy this, we may be able to provide a sufficiently convincing
 experience despite our currently limited ability to meaningfully interpolate
 HRIRs 
\begin_inset CommandInset citation
LatexCommand cite
key "key-7"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename attachments/hrtf_implementation.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:hrtf_implementation"

\end_inset

Simple spatial audio engine, using HRTF only
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Dynamic localisation
\end_layout

\begin_layout Standard
To solve this problem, we would be required to track the user's head in
 terms of rotation and translation, dynamically adjusting the filter in
 use to reflect these values.
 This is not a new idea, and has been attempted in research prior to this
 project, but the techniques used to achieve the required spatial accuracy
 have been cumbersome and generally too expensive for use in the home.
 Here, we explore these approaches, and evaluate their effectiveness.
\end_layout

\begin_layout Itemize
beyerdynamic Headzone Home 5.1 Surround Sound System with Head Tracking.
 These headphones + DSP unit (see Figure )
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
beyerdynamic Headzone Home 5.1 Surround Sound System with Head Tracking.
 (beyerdynamic.com)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "HW38"
key "key-1"

\end_inset

Hans Wallach, 
\begin_inset Quotes eld
\end_inset

On Sound Localisation
\begin_inset Quotes erd
\end_inset

, Program of the Nineteenth Meeting of the Acoustical Society of America,
 May 2 and 3, 1938
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-6"

\end_inset

Sreenivas et al, 
\begin_inset Quotes eld
\end_inset

HEAD RELATED IMPULSE RESPONSE INTERPOLATION FOR DYNAMIC SPATIALIZATION
\begin_inset Quotes erd
\end_inset

, ECE Dept, Indian Institute of Science, Bangalore
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-7"

\end_inset

C.
 I.
 Cheng and G.
 H.
 Wakefield, 
\begin_inset Quotes eld
\end_inset

Introduction to Head-Related Transfer Functions (HRTFs): Representations
 of HRTFs in Time, Frequency and Space, University of Michigan, Department
 of Electrical Engineering and Computer Science, Ann Arbor, MI 48109, USA
 
\end_layout

\end_body
\end_document
